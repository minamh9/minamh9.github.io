<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Mina Mehdinia's Data Science Portfolio Website</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
				<header id="header">
					<a href="index.html" class="logo">Mina Mehdinia</a>
				</header>


				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="index.html">Portfolio</a></li>
							<li class="active"><a href="generic.html">About Me</a></li>
							<li><a href="elements.html">Apps</a></li>
						</ul>
						<ul class="icons">
							<li><a href="https://linkedin.com/in/mina-mehdinia/" class="icon brands alt fa-linkedin"><span class="label">Linkedin</span></a></li>
							<li><a href="https://github.com/minamh9" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
                        <ul class="actions special">
										<li><a href="https://github.com/minamh9/HUGGING-FACE-TRANSFORMERS-AND-BERT/blob/main/Amazon_Review_Transformers.ipynb" class="button">Click here for code</a></li>
									</ul>

							<head>
    <title>Understanding Hugging Face Transformers and BERT</title>
              </head>
              <body>
                  <h1>Understanding Hugging Face Transformers and BERT</h1>
                  <p>In this blog post, we will explore the code that demonstrates how to use Hugging Face Transformers and BERT for sentiment analysis. We will delve into the details of how these powerful tools work and how they can be leveraged to achieve state-of-the-art results in natural language processing tasks.</p>

                  <h2>Introduction to Hugging Face Transformers</h2>
                  <p>Hugging Face Transformers is a library that provides a high-level interface for working with transformer models in natural language processing. It simplifies the process of fine-tuning pre-trained models on specific tasks by providing ready-to-use components and utilities.</p>

                  <h2>Understanding BERT</h2>
                  <p>BERT (Bidirectional Encoder Representations from Transformers) is one of the most popular transformer models in NLP. It is pre-trained on a large corpus of text data using unsupervised learning, which enables it to learn powerful representations of language.</p>
                  <p>BERT uses a transformer architecture, which consists of self-attention mechanisms and feed-forward neural networks. It can capture contextual relationships between words and encode them into dense vector representations, often referred to as word embeddings.</p>

                  <h2>Step-by-Step Explanation of the Code</h2>
                  <p>Let's go through the code step by step to understand how Hugging Face Transformers and BERT are used for sentiment analysis:</p>
                  <ol>
                      <li>First, the necessary libraries and dependencies are installed, including the specific versions of the transformers, torch, and accelerate packages.</li>
                      <li>The Amazon Fashion dataset is loaded from a JSON gzip file and stored in a pandas DataFrame.</li>
                      <li>Some preprocessing steps are performed, such as filter out unverified reviews, drop missing values, adjust the rating scale, and split the data into train and test sets.</li>
                      <li>The BERT tokenizer is initialized using the AutoTokenizer class from the transformers library. We also check the distribution of token counts in the reviews to ensure they're manageable for our model.</li>
                      <div style="text-align: center;">
                      <img src="./images/download.png" alt="Distribution of Number of Tokens in reviewText">
                      </div>
                      <li>A tokenization function is defined to tokenize the reviewText column of the datasets and add the tokenized inputs along with the labels.</li>
                      <li>The tokenization function is applied to both the training and testing datasets using the map() method.</li>
                      <li>The datasets are converted to the 'torch' format using the set_format() method.</li>
                      <li>The BERT model for sequence classification is initialized using the AutoModelForSequenceClassification class.</li>
                      <li>The training arguments are defined using the TrainingArguments class, specifying the output directory, number of epochs, batch sizes, and other configurations.</li>
                      <li>The trainer is instantiated with the BERT model, training arguments, and the training and evaluation datasets.</li>
                      <li>Trainer class from HuggingFace makes training easy. Once our model is trained, we evaluate it on the test set to measure its performance.</li>
                      <li>Predictions are made on the test dataset using the predict() method, and evaluation metrics such as accuracy, precision, recall, F1-score, and ROC AUC are calculated.
                      <li>To better understand the performance of our model for each class, we plot the ROC curve. This gives insights into the trade-offs between the true positive rate and false positive rate for our model.</li>
                      <div style="text-align: center;">
                          <img src="./images/download (1).png" alt="ROC Curve">
                      </div>
                       <table>
                                    <thead>
                                        <tr>
                                            <th>Accuracy</th>
                                            <th>Precision</th>
                                            <th>Recall</th>
                                            <th>F1 Score</th>
                                        </tr>
                                    </thead>
                                    <tbody>
                                        <tr>
                                            <td>0.691</td>
                                            <td>0.664</td>
                                            <td>0.691</td>
                                            <td>0.667</td>
                                        </tr>
                                    </tbody>
                                </table>


                  </ol>

                  <h2>Conclusion</h2>
                  <p>Hugging Face Transformers and BERT provide a powerful and efficient way to utilize pre-trained transformer models for various natural language processing tasks. In this blog post, we have explored the code that demonstrates how to use Hugging Face Transformers and BERT for sentiment analysis. We have seen how to load and preprocess the dataset, initialize the BERT model, train and evaluate the model using the Trainer class, and visualize the performance using ROC curves.</p>
                  <p>With the help of Hugging Face Transformers, researchers and developers can accelerate their NLP projects and achieve state-of-the-art results without starting from scratch. The code presented in this blog post serves as a starting point for building sentiment analysis models using BERT and can be adapted and extended for other NLP tasks as well.</p>

    </div>
    </body>
</html>

